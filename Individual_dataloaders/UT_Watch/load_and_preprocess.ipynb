{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd73031",
   "metadata": {},
   "source": [
    "## UT Watch Data Preprocessing notebook\n",
    "### Step 0. Setup the paths and env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0cc4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths & contracts ready.\n",
      "Schema keys: ['name', 'version', 'primary_index', 'description', 'columns', 'rate_hz', 'axis_frame', 'unit_contract', 'unknown_activity_id', 'expectations']\n",
      "RAW UT_Watch path: /home/aidan/IMU_LM_Data/data/raw_data/UT_Watch/ut_watch_dataset\n",
      "CLEANED: /home/aidan/IMU_LM_Data/data/cleaned_premerge\n",
      "MERGED: /home/aidan/IMU_LM_Data/data/merged_dataset\n",
      "Unknown activity ID: 9000\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# STEP 0: Environment & paths\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, sys, os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT = Path(\"/home/aidan/IMU_LM_Data\")\n",
    "sys.path.insert(0, str(ROOT))  # so UTILS is importable\n",
    "\n",
    "from UTILS.helpers import (\n",
    "    resample_df,      # not strictly needed here but useful to have\n",
    "    convert_unit,\n",
    "    zscore_normalize,\n",
    "    normalize_str,\n",
    "    keyize,\n",
    "    _keyize,\n",
    ")\n",
    "\n",
    "BASE    = ROOT / \"data\"\n",
    "RAW = BASE / \"raw_data\" / \"UT_Watch\" / \"ut_watch_dataset\"\n",
    "CLEANED = BASE / \"cleaned_premerge\"\n",
    "MERGED  = BASE / \"merged_dataset\"\n",
    "\n",
    "SCHEMA_PATH       = ROOT / \"Unification\" / \"schemas\" / \"continuous_stream_schema.json\"\n",
    "ACTIVITY_MAP_PATH = ROOT / \"Unification\" / \"schemas\" / \"activity_mapping.json\"\n",
    "\n",
    "SCHEMA       = json.loads(SCHEMA_PATH.read_text())\n",
    "ACT_MAP_FULL = json.loads(ACTIVITY_MAP_PATH.read_text())\n",
    "\n",
    "UNKNOWN_ID = int(ACT_MAP_FULL.get(\"unknown_activity_id\", -1))\n",
    "ID2NAME    = {int(x[\"id\"]): x[\"name\"] for x in ACT_MAP_FULL[\"label_set\"]}\n",
    "RAW2ID     = {_keyize(k): int(v) for k, v in ACT_MAP_FULL.get(\"mapping\", {}).items()}\n",
    "\n",
    "print(\"Paths & contracts ready.\")\n",
    "print(f\"Schema keys: {list(SCHEMA.keys())}\")\n",
    "print(\"RAW UT_Watch path:\", RAW)\n",
    "print(\"CLEANED:\", CLEANED)\n",
    "print(\"MERGED:\", MERGED)\n",
    "print(\"Unknown activity ID:\", UNKNOWN_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ebe31d",
   "metadata": {},
   "source": [
    "### Step 1. Ingest, preproccess and map the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b61aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 session folders under /home/aidan/IMU_LM_Data/data/raw_data/UT_Watch/ut_watch_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 011: dropping 52906 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:   2%|▎         | 1/40 [00:01<00:42,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 012: dropping 6053 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:   5%|▌         | 2/40 [00:02<00:48,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 021: dropping 3791 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:   8%|▊         | 3/40 [00:03<00:45,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 022: dropping 2223 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  10%|█         | 4/40 [00:04<00:40,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 031: dropping 3715 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  12%|█▎        | 5/40 [00:06<00:44,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 032: dropping 3909 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  15%|█▌        | 6/40 [00:07<00:41,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 041: dropping 6074 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  18%|█▊        | 7/40 [00:08<00:45,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 042: dropping 3571 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  20%|██        | 8/40 [00:10<00:43,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 051: dropping 7290 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  22%|██▎       | 9/40 [00:11<00:44,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 052: dropping 2180 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  25%|██▌       | 10/40 [00:13<00:40,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 061: dropping 8580 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  28%|██▊       | 11/40 [00:14<00:39,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 062: dropping 2969 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  30%|███       | 12/40 [00:15<00:36,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 071: dropping 3368 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  32%|███▎      | 13/40 [00:17<00:39,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 072: dropping 4584 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  35%|███▌      | 14/40 [00:18<00:38,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 081: dropping 3895 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  38%|███▊      | 15/40 [00:20<00:36,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 082: dropping 3599 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  40%|████      | 16/40 [00:21<00:34,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 091: dropping 3891 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  42%|████▎     | 17/40 [00:23<00:31,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 092: dropping 2238 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  45%|████▌     | 18/40 [00:24<00:29,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 101: dropping 3544 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  48%|████▊     | 19/40 [00:25<00:29,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 102: dropping 3452 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  50%|█████     | 20/40 [00:27<00:27,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 111: dropping 4670 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  52%|█████▎    | 21/40 [00:28<00:28,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 112: dropping 3236 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  55%|█████▌    | 22/40 [00:29<00:24,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 121: dropping 4448 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  57%|█████▊    | 23/40 [00:31<00:23,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 122: dropping 2807 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  60%|██████    | 24/40 [00:32<00:21,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 131: dropping 6019 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  62%|██████▎   | 25/40 [00:34<00:20,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 132: dropping 3285 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  65%|██████▌   | 26/40 [00:35<00:19,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 141: dropping 6327 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  68%|██████▊   | 27/40 [00:36<00:18,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 142: dropping 2465 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  70%|███████   | 28/40 [00:38<00:16,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 151: dropping 9931 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  72%|███████▎  | 29/40 [00:40<00:17,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trim] 152: dropping 2455 tail samples beyond last label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  92%|█████████▎| 37/40 [02:03<00:32, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] No IMU file found for session 'LV2'\n",
      "[note] Using IMU file 'SK2.csv' for session 'SK1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions:  98%|█████████▊| 39/40 [02:15<00:08,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[note] Using IMU file 'SK3.csv' for session 'SK2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sessions: 100%|██████████| 40/40 [02:28<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== UT_Watch RAW SUMMARY ===\n",
      "Shape: 8,146,709 rows × 10 cols\n",
      "Subjects: 20 | Sessions: 4 | Raw classes: 24\n",
      "Median Hz across sessions: 50.00\n",
      "Median session duration (s): 1666.30\n",
      "\n",
      "Top-15 raw labels:\n",
      "  none                                          7,004,993\n",
      "  Microwave                                     56,282\n",
      "  Sweeping                                      52,144\n",
      "  Frying                                        50,984\n",
      "  Drinking                                      50,873\n",
      "  Writing                                       50,500\n",
      "  Dishes                                        49,881\n",
      "  Eating                                        49,865\n",
      "  Drawing                                       49,828\n",
      "  Keyboard                                      49,748\n",
      "  Mobile                                        49,443\n",
      "  Vacuuming                                     49,195\n",
      "  Browsing                                      49,176\n",
      "  Scratching                                    49,115\n",
      "  Teeth                                         49,075\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# STEP 1: Ingest UT_Watch raw IMU + annotations\n",
    "# ======================================================================\n",
    "\n",
    "def parse_ut_watch_ids(session_code: str) -> tuple[str, str]:\n",
    "    session_code = str(session_code).strip()\n",
    "\n",
    "    # Numeric 3-digit code: xxy where xx is participant, y is session (semi-naturalistic)\n",
    "    if re.fullmatch(r\"\\d{3}\", session_code):\n",
    "        subj_num = int(session_code[:2])\n",
    "        sess_num = int(session_code[2])\n",
    "        subject_id = f\"{subj_num:02d}\"\n",
    "        session_id = f\"S{sess_num:d}\"\n",
    "        return subject_id, session_id\n",
    "\n",
    "    # Two-letter + digit (e.g., HH2, AB1) for in-the-wild data\n",
    "    m = re.fullmatch(r\"([A-Za-z]{2})(\\d)\", session_code)\n",
    "    if m:\n",
    "        subj_code, sess_num = m.groups()\n",
    "        subject_id = subj_code.upper()\n",
    "        # mark explicitly as wild:\n",
    "        session_id = f\"wild_S{sess_num}\"\n",
    "        return subject_id, session_id\n",
    "\n",
    "    # Fallback\n",
    "    subject_id = f\"U{abs(hash(session_code)) % 1000:03d}\"\n",
    "    session_id = session_code\n",
    "    return subject_id, session_id\n",
    "\n",
    "\n",
    "def load_ut_watch_raw(raw_root: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load UT_Watch IMU + annotation files and return a 'raw' dataframe:\n",
    "\n",
    "        subject_id, session_id, timestamp_s,\n",
    "        acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z,\n",
    "        activity_label_raw\n",
    "\n",
    "    - Assumes IMU is sampled at 50 Hz.\n",
    "    - Builds synthetic timestamps aligned to annotation Start (ms) when available.\n",
    "    \"\"\"\n",
    "    dataset_name = \"ut_watch\"\n",
    "    rows = []\n",
    "\n",
    "    imu_longer_map = {}\n",
    "    imu_shorter_map = {}\n",
    "    trimmed_tail_map = {}\n",
    "\n",
    "    session_dirs = [p for p in sorted(raw_root.iterdir()) if p.is_dir()]\n",
    "    print(f\"Found {len(session_dirs)} session folders under {raw_root}\")\n",
    "\n",
    "    for session_dir in tqdm(session_dirs, desc=\"Sessions\"):\n",
    "        session_code = session_dir.name\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1) Locate IMU file (011.csv or 011.tab, or fallback *.csv at root)\n",
    "        # ------------------------------------------------------------------\n",
    "        imu_candidates = [\n",
    "            session_dir / f\"{session_code}.csv\",\n",
    "            session_dir / f\"{session_code}.tab\",\n",
    "        ]\n",
    "        motion_file = next((p for p in imu_candidates if p.exists()), None)\n",
    "\n",
    "        if motion_file is None:\n",
    "            # Fallback: any CSV at TOP LEVEL that is not an annotation file\n",
    "            fallback = [\n",
    "                p for p in sorted(session_dir.glob(\"*.csv\"))\n",
    "                if not (p.name.endswith(\"ant.csv\") or p.name.endswith(\"_ann.csv\"))\n",
    "            ]\n",
    "            if fallback:\n",
    "                motion_file = fallback[0]\n",
    "                print(f\"[note] Using IMU file '{motion_file.name}' for session '{session_code}'\")\n",
    "\n",
    "        if motion_file is None:\n",
    "            print(f\"[skip] No IMU file found for session '{session_code}'\")\n",
    "            continue\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2) Locate annotation file at ROOT ONLY\n",
    "        #    - 011ant.csv / 011ant.tab (semi-naturalistic)\n",
    "        #    - HH1_ann.csv             (in-the-wild)\n",
    "        # ------------------------------------------------------------------\n",
    "        ann_candidates = [\n",
    "            session_dir / f\"{session_code}ant.csv\",\n",
    "            session_dir / f\"{session_code}ant.tab\",\n",
    "            session_dir / f\"{session_code}_ann.csv\",\n",
    "        ]\n",
    "        ann_file = next((p for p in ann_candidates if p.exists()), None)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3) Read IMU (handle one-line epoch header + 6-column body)\n",
    "        # ------------------------------------------------------------------\n",
    "        try:\n",
    "            # check first line: if it's a lone epoch number, skip it\n",
    "            with motion_file.open(\"r\") as f:\n",
    "                first_line = f.readline().strip()\n",
    "\n",
    "            if first_line and (\",\" not in first_line) and (\"\\t\" not in first_line):\n",
    "                skiprows = 1\n",
    "            else:\n",
    "                skiprows = 0\n",
    "            \n",
    "\n",
    "            if motion_file.suffix == \".csv\":\n",
    "                imu_df = pd.read_csv(motion_file, header=None, skiprows=skiprows)\n",
    "            else:\n",
    "                imu_df = pd.read_csv(motion_file, sep=\"\\t\", header=None, skiprows=skiprows)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Reading IMU '{motion_file}': {e}\")\n",
    "            continue\n",
    "\n",
    "        # extra guard – fine to keep or drop\n",
    "        if imu_df.shape[1] >= 1:\n",
    "            fr = imu_df.iloc[0]\n",
    "            if fr.iloc[0] == fr.iloc[0] and fr.iloc[1:].isna().all():\n",
    "                imu_df = imu_df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "        if imu_df.shape[1] < 6:\n",
    "            print(f\"[skip] {session_code}: IMU data has <6 columns ({imu_df.shape[1]})\")\n",
    "            continue\n",
    "\n",
    "        imu_df = imu_df.iloc[:, :6].copy()\n",
    "        imu_df.columns = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "\n",
    "        n = len(imu_df)\n",
    "        if n == 0:\n",
    "            print(f\"[skip] {session_code}: empty IMU file\")\n",
    "            continue\n",
    "\n",
    "        # Parse subject/session IDs\n",
    "        subject_id, session_id = parse_ut_watch_ids(session_code)\n",
    "\n",
    "        # Default raw label\n",
    "        imu_df[\"activity_label_raw\"] = \"none\"\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 4) Read annotations & build timestamps at 50 Hz\n",
    "        # ------------------------------------------------------------------\n",
    "        ann_df = None\n",
    "        if ann_file is not None:\n",
    "            try:\n",
    "                if ann_file.suffix == \".csv\":\n",
    "                    # semi-naturalistic 011ant.csv is actually TAB-separated\n",
    "                    if ann_file.name.endswith(\"ant.csv\"):\n",
    "                        ann_df = pd.read_csv(ann_file, sep=\"\\t\")\n",
    "                    else:\n",
    "                        # in-the-wild HH1_ann.csv etc. are real CSV\n",
    "                        ann_df = pd.read_csv(ann_file)\n",
    "                else:\n",
    "                    ann_df = pd.read_csv(ann_file, sep=\"\\t\")\n",
    "\n",
    "                ann_df.columns = [c.strip() for c in ann_df.columns]\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] Cannot read annotation '{ann_file}': {e}\")\n",
    "                ann_df = None\n",
    "\n",
    "        # timestamps in milliseconds\n",
    "        if ann_df is not None and \"Start (ms)\" in ann_df.columns and \"End (ms)\" in ann_df.columns:\n",
    "            # semi-naturalistic branch: align to video-derived start time\n",
    "            try:\n",
    "                start0_ms = float(ann_df[\"Start (ms)\"].min())\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] Cannot determine Start (ms) for {session_code}: {e}\")\n",
    "                start0_ms = 0.0\n",
    "\n",
    "            dt_ms = 20.0  # 50 Hz\n",
    "            ts_ms = start0_ms + np.arange(n, dtype=\"float64\") * dt_ms\n",
    "            imu_df[\"timestamp_ms\"] = ts_ms\n",
    "\n",
    "            # Only do this for numeric sessions 011, 012, ... (session_id \"S1\", \"S2\", ...)\n",
    "            if session_id.startswith(\"S\"):\n",
    "                last_end_ms = float(ann_df[\"End (ms)\"].max())\n",
    "                keep_mask = imu_df[\"timestamp_ms\"] <= (last_end_ms + 1e-6)\n",
    "                dropped = int(len(imu_df) - keep_mask.sum())\n",
    "                if dropped > 0:\n",
    "                    print(f\"[trim] {session_code}: dropping {dropped} tail samples beyond last label\")\n",
    "                imu_df = imu_df.loc[keep_mask].reset_index(drop=True)\n",
    "                # refresh timestamp array and n\n",
    "                ts_ms = imu_df[\"timestamp_ms\"].to_numpy()\n",
    "                n = len(imu_df)\n",
    "\n",
    "            # Duration comparison IMU vs annotations (diagnostic only)\n",
    "            imu_range = ts_ms[-1] - ts_ms[0]\n",
    "            ann_range = float(ann_df[\"End (ms)\"].max() - ann_df[\"Start (ms)\"].min())\n",
    "            if imu_range > ann_range + 5000:\n",
    "                imu_longer_map[session_code] = (imu_range - ann_range) / 1000.0\n",
    "            elif imu_range < ann_range - 5000:\n",
    "                imu_shorter_map[session_code] = (ann_range - imu_range) / 1000.0\n",
    "\n",
    "            # Apply labels in [Start, End]\n",
    "            for _, r in ann_df.iterrows():\n",
    "                try:\n",
    "                    s = float(r[\"Start (ms)\"])\n",
    "                    e = float(r[\"End (ms)\"])\n",
    "                    lbl = str(r[\"Label\"])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                mask = (ts_ms >= s) & (ts_ms <= e)\n",
    "                imu_df.loc[mask, \"activity_label_raw\"] = lbl\n",
    "\n",
    "        else:\n",
    "            # No usable Start/End → synthetic timeline, unlabeled (\"none\")\n",
    "            dt_ms = 20.0\n",
    "            ts_ms = np.arange(n, dtype=\"float64\") * dt_ms\n",
    "            imu_df[\"timestamp_ms\"] = ts_ms\n",
    "\n",
    "        # Convert to seconds relative to session start\n",
    "        imu_df[\"timestamp_s\"] = (imu_df[\"timestamp_ms\"] - imu_df[\"timestamp_ms\"].iloc[0]) / 1000.0\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 5) Accumulate rows\n",
    "        # ------------------------------------------------------------------\n",
    "        rows.extend(\n",
    "            {\n",
    "                \"subject_id\": subject_id,\n",
    "                \"session_id\": session_id,\n",
    "                \"timestamp_s\": float(imu_df.at[k, \"timestamp_s\"]),\n",
    "                \"acc_x\": float(imu_df.at[k, \"acc_x\"]),\n",
    "                \"acc_y\": float(imu_df.at[k, \"acc_y\"]),\n",
    "                \"acc_z\": float(imu_df.at[k, \"acc_z\"]),\n",
    "                \"gyro_x\": float(imu_df.at[k, \"gyro_x\"]),\n",
    "                \"gyro_y\": float(imu_df.at[k, \"gyro_y\"]),\n",
    "                \"gyro_z\": float(imu_df.at[k, \"gyro_z\"]),\n",
    "                \"activity_label_raw\": str(imu_df.at[k, \"activity_label_raw\"]),\n",
    "            }\n",
    "            for k in range(n)\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # Combine and summarize\n",
    "    # ----------------------------------------------------------------------\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    print(\"\\n=== UT_Watch RAW SUMMARY ===\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows × {df.shape[1]} cols\")\n",
    "\n",
    "    if len(df):\n",
    "        def est_hz(ts: pd.Series) -> float:\n",
    "            arr = ts.to_numpy()\n",
    "            if arr.size < 3:\n",
    "                return np.nan\n",
    "            dt = np.diff(arr)\n",
    "            dt = dt[(dt > 0) & np.isfinite(dt)]\n",
    "            return float(np.median(1.0 / dt)) if dt.size else np.nan\n",
    "\n",
    "        def sess_dur(ts: pd.Series) -> float:\n",
    "            arr = ts.to_numpy()\n",
    "            return float(arr[-1] - arr[0]) if arr.size > 1 else 0.0\n",
    "\n",
    "        hz = df.groupby([\"subject_id\", \"session_id\"])[\"timestamp_s\"].apply(est_hz)\n",
    "        dur = df.groupby([\"subject_id\", \"session_id\"])[\"timestamp_s\"].apply(sess_dur)\n",
    "\n",
    "        n_subjects = df[\"subject_id\"].nunique()\n",
    "        n_sessions = df[\"session_id\"].nunique()\n",
    "        n_classes = df[\"activity_label_raw\"].astype(str).str.lower().nunique()\n",
    "\n",
    "        print(f\"Subjects: {n_subjects} | Sessions: {n_sessions} | Raw classes: {n_classes}\")\n",
    "        print(f\"Median Hz across sessions: {np.nanmedian(hz.values):.2f}\")\n",
    "        print(f\"Median session duration (s): {np.nanmedian(dur.values):.2f}\")\n",
    "\n",
    "        top = df[\"activity_label_raw\"].value_counts().head(15)\n",
    "        print(\"\\nTop-15 raw labels:\")\n",
    "        for lbl, cnt in top.items():\n",
    "            print(f\"  {lbl:45s} {cnt:,}\")\n",
    "\n",
    "    if imu_longer_map:\n",
    "        print(\"\\nIMU longer than annotations:\")\n",
    "        for sess, diff in imu_longer_map.items():\n",
    "            print(f\"  {sess}: {diff:.2f} s longer\")\n",
    "    if imu_shorter_map:\n",
    "        print(\"\\nIMU shorter than annotations:\")\n",
    "        for sess, diff in imu_shorter_map.items():\n",
    "            print(f\"  {sess}: {diff:.2f} s shorter\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Run raw ingest\n",
    "ut_raw_df = load_ut_watch_raw(RAW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ab432",
   "metadata": {},
   "source": [
    "### Step 2. Map the data and audit the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89ef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label sanity: null=0 | empty=0\n",
      "Raw label unique: 24 | Unmapped: 1\n",
      "Unmapped (top-10 by frequency):\n",
      "raw_label  count\n",
      " clapping  49019\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_label</th>\n",
       "      <th>count</th>\n",
       "      <th>mapped_id</th>\n",
       "      <th>mapped_nm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>7004993</td>\n",
       "      <td>0</td>\n",
       "      <td>rest_inactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microwave</td>\n",
       "      <td>56282</td>\n",
       "      <td>15</td>\n",
       "      <td>adl_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sweeping</td>\n",
       "      <td>52144</td>\n",
       "      <td>13</td>\n",
       "      <td>adl_household_general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frying</td>\n",
       "      <td>50984</td>\n",
       "      <td>15</td>\n",
       "      <td>adl_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drinking</td>\n",
       "      <td>50873</td>\n",
       "      <td>15</td>\n",
       "      <td>adl_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>writing</td>\n",
       "      <td>50500</td>\n",
       "      <td>14</td>\n",
       "      <td>adl_desk_device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dishes</td>\n",
       "      <td>49881</td>\n",
       "      <td>13</td>\n",
       "      <td>adl_household_general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eating</td>\n",
       "      <td>49865</td>\n",
       "      <td>15</td>\n",
       "      <td>adl_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>drawing</td>\n",
       "      <td>49828</td>\n",
       "      <td>14</td>\n",
       "      <td>adl_desk_device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>keyboard</td>\n",
       "      <td>49748</td>\n",
       "      <td>14</td>\n",
       "      <td>adl_desk_device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mobile</td>\n",
       "      <td>49443</td>\n",
       "      <td>14</td>\n",
       "      <td>adl_desk_device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vacuuming</td>\n",
       "      <td>49195</td>\n",
       "      <td>13</td>\n",
       "      <td>adl_household_general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>browsing</td>\n",
       "      <td>49176</td>\n",
       "      <td>14</td>\n",
       "      <td>adl_desk_device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>scratching</td>\n",
       "      <td>49115</td>\n",
       "      <td>19</td>\n",
       "      <td>adl_personal_care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>teeth</td>\n",
       "      <td>49075</td>\n",
       "      <td>19</td>\n",
       "      <td>adl_personal_care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>clapping</td>\n",
       "      <td>49019</td>\n",
       "      <td>9000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wiping</td>\n",
       "      <td>48990</td>\n",
       "      <td>13</td>\n",
       "      <td>adl_household_general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>grating</td>\n",
       "      <td>48935</td>\n",
       "      <td>15</td>\n",
       "      <td>adl_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hair</td>\n",
       "      <td>48901</td>\n",
       "      <td>19</td>\n",
       "      <td>adl_personal_care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chopping</td>\n",
       "      <td>48869</td>\n",
       "      <td>15</td>\n",
       "      <td>adl_food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     raw_label    count  mapped_id              mapped_nm\n",
       "0         none  7004993          0          rest_inactive\n",
       "1    microwave    56282         15               adl_food\n",
       "2     sweeping    52144         13  adl_household_general\n",
       "3       frying    50984         15               adl_food\n",
       "4     drinking    50873         15               adl_food\n",
       "5      writing    50500         14        adl_desk_device\n",
       "6       dishes    49881         13  adl_household_general\n",
       "7       eating    49865         15               adl_food\n",
       "8      drawing    49828         14        adl_desk_device\n",
       "9     keyboard    49748         14        adl_desk_device\n",
       "10      mobile    49443         14        adl_desk_device\n",
       "11   vacuuming    49195         13  adl_household_general\n",
       "12    browsing    49176         14        adl_desk_device\n",
       "13  scratching    49115         19      adl_personal_care\n",
       "14       teeth    49075         19      adl_personal_care\n",
       "15    clapping    49019       9000                  other\n",
       "16      wiping    48990         13  adl_household_general\n",
       "17     grating    48935         15               adl_food\n",
       "18        hair    48901         19      adl_personal_care\n",
       "19    chopping    48869         15               adl_food"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# STEP 2: Activity mapping audit (raw → global ontology) bigger for Uwatch\n",
    "# ======================================================================\n",
    "\n",
    "# 1) Quick sanity on the raw labels *before* normalization\n",
    "n_null  = ut_raw_df[\"activity_label_raw\"].isna().sum()\n",
    "n_empty = (ut_raw_df[\"activity_label_raw\"].astype(str).str.strip() == \"\").sum()\n",
    "print(f\"Label sanity: null={n_null:,} | empty={n_empty:,}\")\n",
    "\n",
    "# 2) Normalize labels for mapping\n",
    "norm_raw = (\n",
    "    ut_raw_df[\"activity_label_raw\"]\n",
    "      .astype(str)\n",
    "      .fillna(\"none\")\n",
    "      .map(_keyize)   # lower, strip, etc.\n",
    ")\n",
    "\n",
    "raw_counts = (\n",
    "    norm_raw\n",
    "      .value_counts()\n",
    "      .rename_axis(\"raw_label\")\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# 3) Map to global IDs/names\n",
    "raw_counts[\"mapped_id\"] = (\n",
    "    raw_counts[\"raw_label\"]\n",
    "      .map(RAW2ID)\n",
    "      .fillna(UNKNOWN_ID)\n",
    "      .astype(int)\n",
    ")\n",
    "\n",
    "raw_counts[\"mapped_nm\"] = raw_counts[\"mapped_id\"].map(\n",
    "    lambda x: ID2NAME.get(int(x), \"other\")\n",
    ")\n",
    "\n",
    "# 4) Inspect unmapped labels\n",
    "unmapped = raw_counts.loc[raw_counts[\"mapped_id\"] == UNKNOWN_ID]\n",
    "\n",
    "print(f\"Raw label unique: {len(raw_counts)} | Unmapped: {len(unmapped)}\")\n",
    "print(\"Unmapped (top-10 by frequency):\")\n",
    "print(\n",
    "    unmapped.nlargest(10, \"count\")[[\"raw_label\", \"count\"]]\n",
    "      .to_string(index=False)\n",
    ")\n",
    "\n",
    "raw_counts.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d415a1df",
   "metadata": {},
   "source": [
    "### Step 3. Build and clean dataset in stream json fromat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa1fa766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp_ns</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>global_activity_id</th>\n",
       "      <th>global_activity_label</th>\n",
       "      <th>dataset_activity_id</th>\n",
       "      <th>dataset_activity_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ut_watch</td>\n",
       "      <td>01</td>\n",
       "      <td>S1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.558</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>9.715</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>14</td>\n",
       "      <td>adl_desk_device</td>\n",
       "      <td>22</td>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut_watch</td>\n",
       "      <td>01</td>\n",
       "      <td>S1</td>\n",
       "      <td>20000000</td>\n",
       "      <td>-1.558</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>9.715</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>14</td>\n",
       "      <td>adl_desk_device</td>\n",
       "      <td>22</td>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ut_watch</td>\n",
       "      <td>01</td>\n",
       "      <td>S1</td>\n",
       "      <td>40000000</td>\n",
       "      <td>-1.642</td>\n",
       "      <td>0.042</td>\n",
       "      <td>9.705</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>14</td>\n",
       "      <td>adl_desk_device</td>\n",
       "      <td>22</td>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ut_watch</td>\n",
       "      <td>01</td>\n",
       "      <td>S1</td>\n",
       "      <td>60000000</td>\n",
       "      <td>-1.508</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>9.775</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>14</td>\n",
       "      <td>adl_desk_device</td>\n",
       "      <td>22</td>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ut_watch</td>\n",
       "      <td>01</td>\n",
       "      <td>S1</td>\n",
       "      <td>80000000</td>\n",
       "      <td>-1.544</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>9.736</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>14</td>\n",
       "      <td>adl_desk_device</td>\n",
       "      <td>22</td>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset subject_id session_id  timestamp_ns  acc_x  acc_y  acc_z  gyro_x  \\\n",
       "0  ut_watch         01         S1             0 -1.558 -0.140  9.715   0.022   \n",
       "1  ut_watch         01         S1      20000000 -1.558 -0.140  9.715   0.013   \n",
       "2  ut_watch         01         S1      40000000 -1.642  0.042  9.705  -0.035   \n",
       "3  ut_watch         01         S1      60000000 -1.508 -0.137  9.775  -0.001   \n",
       "4  ut_watch         01         S1      80000000 -1.544 -0.089  9.736   0.033   \n",
       "\n",
       "   gyro_y  gyro_z  global_activity_id global_activity_label  \\\n",
       "0   0.008   0.006                  14       adl_desk_device   \n",
       "1   0.012   0.009                  14       adl_desk_device   \n",
       "2   0.001   0.004                  14       adl_desk_device   \n",
       "3   0.004   0.004                  14       adl_desk_device   \n",
       "4   0.002   0.002                  14       adl_desk_device   \n",
       "\n",
       "   dataset_activity_id dataset_activity_label  \n",
       "0                   22                Writing  \n",
       "1                   22                Writing  \n",
       "2                   22                Writing  \n",
       "3                   22                Writing  \n",
       "4                   22                Writing  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# STEP 3: Convert UT_Watch → continuous_stream schema\n",
    "# ======================================================================\n",
    "\n",
    "def to_continuous_stream_ut(df_raw: pd.DataFrame, dataset_name: str = \"ut_watch\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert UT_Watch raw frame into the unified continuous_stream schema.\n",
    "    \"\"\"\n",
    "    if df_raw.empty:\n",
    "        return pd.DataFrame(columns=[c[\"name\"] for c in SCHEMA[\"columns\"]])\n",
    "\n",
    "    # ---------- GLOBAL (ontology) ----------\n",
    "    raw_key = df_raw[\"activity_label_raw\"].astype(str).map(_keyize)\n",
    "    gid = raw_key.map(RAW2ID).fillna(UNKNOWN_ID).astype(\"int16\")\n",
    "    glabel = gid.map(lambda x: ID2NAME.get(int(x), \"other\")).astype(\"string\")\n",
    "\n",
    "    # ---------- NATIVE (dataset-specific) ----------\n",
    "    # One stable ID per distinct raw label\n",
    "    unique_labels = sorted(df_raw[\"activity_label_raw\"].astype(str).unique())\n",
    "    label2id = {lbl: idx for idx, lbl in enumerate(unique_labels)}\n",
    "    native_id = df_raw[\"activity_label_raw\"].astype(str).map(label2id).astype(\"Int16\")\n",
    "    native_lbl = df_raw[\"activity_label_raw\"].astype(\"string\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"dataset\":        dataset_name,\n",
    "        \"subject_id\":     df_raw[\"subject_id\"].astype(\"string\"),\n",
    "        \"session_id\":     df_raw[\"session_id\"].astype(\"string\"),\n",
    "        \"timestamp_ns\":   (df_raw[\"timestamp_s\"].astype(np.float64) * 1e9).round().astype(\"int64\"),\n",
    "\n",
    "        \"acc_x\": df_raw[\"acc_x\"].astype(\"float32\"),\n",
    "        \"acc_y\": df_raw[\"acc_y\"].astype(\"float32\"),\n",
    "        \"acc_z\": df_raw[\"acc_z\"].astype(\"float32\"),\n",
    "        \"gyro_x\": df_raw[\"gyro_x\"].astype(\"float32\"),\n",
    "        \"gyro_y\": df_raw[\"gyro_y\"].astype(\"float32\"),\n",
    "        \"gyro_z\": df_raw[\"gyro_z\"].astype(\"float32\"),\n",
    "\n",
    "        \"global_activity_id\":    gid,\n",
    "        \"global_activity_label\": glabel,\n",
    "\n",
    "        \"dataset_activity_id\":   native_id,\n",
    "        \"dataset_activity_label\": native_lbl,\n",
    "    })\n",
    "\n",
    "    order = [c[\"name\"] for c in SCHEMA[\"columns\"]]\n",
    "    return out[order]\n",
    "\n",
    "\n",
    "ut_watch_df = to_continuous_stream_ut(ut_raw_df, dataset_name=\"ut_watch\")\n",
    "ut_watch_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c61e6",
   "metadata": {},
   "source": [
    "### Step 4. Audit check the unified frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1109af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIFIED UT_Watch rows: 8146709\n",
      "Subjects: 20 | Sessions: 4\n",
      "Monotonic violations (groups): 0\n",
      "Median Hz: 50.00 (target=50)\n",
      "Rows meeting required-not-null: 100.00%\n",
      "\n",
      "Top-10 canonical labels:\n",
      "global_activity_label\n",
      "rest_inactive            7004993\n",
      "adl_food                  402387\n",
      "adl_desk_device           248695\n",
      "adl_household_general     248443\n",
      "adl_personal_care         147091\n",
      "other                      49019\n",
      "walk                       46081\n",
      "Name: count, dtype: Int64\n",
      "Global mapping coverage: 99.4% (unknown=9000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "global_activity_label\n",
       "rest_inactive            7004993\n",
       "adl_food                  402387\n",
       "adl_desk_device           248695\n",
       "adl_household_general     248443\n",
       "adl_personal_care         147091\n",
       "other                      49019\n",
       "walk                       46081\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# STEP 4: Audit unified UT_Watch frame\n",
    "# ======================================================================\n",
    "\n",
    "print(\"UNIFIED UT_Watch rows:\", len(ut_watch_df))\n",
    "print(\"Subjects:\", ut_watch_df[\"subject_id\"].nunique(),\n",
    "      \"| Sessions:\", ut_watch_df[\"session_id\"].nunique())\n",
    "\n",
    "# Monotonic timestamps per (subject, session)\n",
    "viol = 0\n",
    "for (_sid, _sess), g in ut_watch_df.groupby([\"subject_id\", \"session_id\"], sort=False):\n",
    "    ts = g[\"timestamp_ns\"].to_numpy()\n",
    "    if ts.size and not np.all(np.diff(ts) >= 0):\n",
    "        viol += 1\n",
    "print(\"Monotonic violations (groups):\", viol)\n",
    "\n",
    "\n",
    "def est_hz_ns(ts_ns: pd.Series) -> float:\n",
    "    arr = ts_ns.to_numpy()\n",
    "    if arr.size < 3:\n",
    "        return np.nan\n",
    "    dt = np.diff(arr) / 1e9  # ns → s\n",
    "    dt = dt[(dt > 0) & np.isfinite(dt)]\n",
    "    return float(np.median(1.0 / dt)) if dt.size else np.nan\n",
    "\n",
    "\n",
    "hz = ut_watch_df.groupby([\"subject_id\", \"session_id\"])[\"timestamp_ns\"].apply(est_hz_ns)\n",
    "print(f\"Median Hz: {np.nanmedian(hz.values):.2f} (target={SCHEMA['rate_hz']})\")\n",
    "\n",
    "# required-not-null coverage\n",
    "req = SCHEMA[\"expectations\"][\"required_not_null\"]\n",
    "pct = ut_watch_df[req].notnull().all(axis=1).mean() * 100\n",
    "print(f\"Rows meeting required-not-null: {pct:.2f}%\")\n",
    "\n",
    "print(\"\\nTop-10 canonical labels:\")\n",
    "print(ut_watch_df[\"global_activity_label\"].value_counts().head(10))\n",
    "\n",
    "cov = (ut_watch_df[\"global_activity_id\"] != UNKNOWN_ID).mean() * 100\n",
    "print(f\"Global mapping coverage: {cov:.1f}% (unknown={UNKNOWN_ID})\")\n",
    "\n",
    "ut_watch_df[\"global_activity_label\"].value_counts().head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1d877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(ut_watch_df[\"session_id\"].unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c570fa8",
   "metadata": {},
   "source": [
    "### Step 5. Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# STEP 5: Save unified UT_Watch frame\n",
    "# ======================================================================\n",
    "\n",
    "CLEANED.mkdir(parents=True, exist_ok=True)\n",
    "out_path = CLEANED / \"ut_watch_clean_data.parquet\"\n",
    "ut_watch_df.to_parquet(out_path, index=False)\n",
    "print(\"Saved UT_Watch continuous_stream frame to:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".IMU_Data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
