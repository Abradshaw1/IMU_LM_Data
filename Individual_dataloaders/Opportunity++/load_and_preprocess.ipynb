{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1839cd8a",
   "metadata": {},
   "source": [
    "## Opportunity++ Data Preprocessing notebook\n",
    "### Step 0. Setup the paths and env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83ad311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths & contracts ready.\n",
      "RAW dir : /home/aidan/IMU_LM_Data/data/raw_data/Opportunity++/dataset\n",
      "CLEANED : /home/aidan/IMU_LM_Data/data/cleaned_premerge\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STEP 0 — Setup & contracts\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "import json, sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT = Path(\"/home/aidan/IMU_LM_Data\")\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from UTILS.helpers import (\n",
    "    read_opp_column_names,\n",
    "    canonicalize_opp_columns,\n",
    "    parse_opp_subject_session,\n",
    "    nearest_label_join_1d,\n",
    "    upsample_df_rate,\n",
    "    _canon,\n",
    "    \n",
    ")\n",
    "\n",
    "BASE      = ROOT / \"data\"\n",
    "RAW_OPP   = BASE / \"raw_data\" / \"Opportunity++\" / \"dataset\"   # *.dat + column_names.txt\n",
    "CLEANED   = BASE / \"cleaned_premerge\"\n",
    "\n",
    "SCHEMA_PATH       = ROOT / \"Unification\" / \"schemas\" / \"continuous_stream_schema.json\"\n",
    "ACTIVITY_MAP_PATH = ROOT / \"Unification\" / \"schemas\" / \"activity_mapping.json\"\n",
    "\n",
    "SCHEMA       = json.loads(SCHEMA_PATH.read_text())\n",
    "ACT_MAP_FULL = json.loads(ACTIVITY_MAP_PATH.read_text())\n",
    "\n",
    "UNKNOWN_ID = int(ACT_MAP_FULL.get(\"unknown_activity_id\", 9000))\n",
    "TARGET_HZ  = int(SCHEMA.get(\"rate_hz\", 50))\n",
    "\n",
    "# activity_mapping.json provides RAW2ID keyed by canonical label strings\n",
    "# RAW2ID = { _canon(k): int(v) for k, v in ACT_MAP_FULL.get(\"mapping\", {}).items() }\n",
    "# ID2NAME = { int(x[\"id\"]): x[\"name\"] for x in ACT_MAP_FULL[\"label_set\"] }\n",
    "\n",
    "print(\"Paths & contracts ready.\")\n",
    "print(\"RAW dir :\", RAW_OPP)\n",
    "print(\"CLEANED :\", CLEANED)\n",
    "\n",
    "# ---- Locomotion (native, used verbatim) ----\n",
    "# Keep the native IDs distinct; labels are simple canonical strings.\n",
    "OPP_LOCO = {\n",
    "    1: \"stand\",\n",
    "    2: \"walk\",\n",
    "    4: \"sit\",\n",
    "    5: \"lie\",\n",
    "}\n",
    "\n",
    "# ---- ML_Both_Arms (native → collapsed label) ----\n",
    "# We KEEP every native ML code in dataset_activity_id (e.g., 406516 vs 406517)\n",
    "# but COLLAPSE their human-readable label to a canonical verb+object (e.g., \"open_door\").\n",
    "OPP_ML = {\n",
    "    406516: \"open_door\",        406517: \"open_door\",\n",
    "    404516: \"close_door\",       404517: \"close_door\",\n",
    "    406520: \"open_fridge\",      404520: \"close_fridge\",\n",
    "    406505: \"open_dishwasher\",  404505: \"close_dishwasher\",\n",
    "    406519: \"open_drawer\",      404519: \"close_drawer\",\n",
    "    406511: \"open_drawer\",      404511: \"close_drawer\",\n",
    "    406508: \"open_drawer\",      404508: \"close_drawer\",\n",
    "    408512: \"clean_table\",\n",
    "    407521: \"drink_from_cup\",\n",
    "    405506: \"toggle_switch\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7229c11b",
   "metadata": {},
   "source": [
    "### Step 1. Ingest, preporccess and map the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bebdc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[OPP] files: 100%|██████████| 24/24 [00:49<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped accel-empty sessions: [('S1', 'DRILL')] | removed_rows=52,130\n",
      "Accel NaNs present (kept for later interpolation): {'acc_x': True, 'acc_y': True, 'acc_z': True}\n",
      "\n",
      "=== RAW SUMMARY (Opportunity++ wrist: loco + ML only) ===\n",
      "Rows: 656,031\n",
      "Median native Hz: 30.30\n",
      "\n",
      "Top native labels (collapsed names, native IDs retained):\n",
      "dataset_activity_label\n",
      "stand               174367\n",
      "walk                144018\n",
      "sit                 105692\n",
      "drink_from_cup       45901\n",
      "lie                  25395\n",
      "open_door            25026\n",
      "close_door           23065\n",
      "open_drawer          20906\n",
      "close_drawer         18866\n",
      "open_fridge          16288\n",
      "close_fridge         14595\n",
      "clean_table          13833\n",
      "open_dishwasher       9698\n",
      "toggle_switch         9465\n",
      "close_dishwasher      8916\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp_s</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>locomotion</th>\n",
       "      <th>ml_both_arms</th>\n",
       "      <th>dataset_activity_id</th>\n",
       "      <th>dataset_activity_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>opportunity++</td>\n",
       "      <td>S1</td>\n",
       "      <td>ADL1</td>\n",
       "      <td>98.466</td>\n",
       "      <td>-4.069760</td>\n",
       "      <td>8.276813</td>\n",
       "      <td>1.470997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>stand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opportunity++</td>\n",
       "      <td>S1</td>\n",
       "      <td>ADL1</td>\n",
       "      <td>98.499</td>\n",
       "      <td>-4.628739</td>\n",
       "      <td>8.963278</td>\n",
       "      <td>1.451384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>stand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>opportunity++</td>\n",
       "      <td>S1</td>\n",
       "      <td>ADL1</td>\n",
       "      <td>98.532</td>\n",
       "      <td>-4.148213</td>\n",
       "      <td>8.521979</td>\n",
       "      <td>1.735777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>stand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset subject_id session_id  timestamp_s     acc_x     acc_y  \\\n",
       "0  opportunity++         S1       ADL1       98.466 -4.069760  8.276813   \n",
       "1  opportunity++         S1       ADL1       98.499 -4.628739  8.963278   \n",
       "2  opportunity++         S1       ADL1       98.532 -4.148213  8.521979   \n",
       "\n",
       "      acc_z  gyro_x  gyro_y  gyro_z  locomotion  ml_both_arms  \\\n",
       "0  1.470997     NaN     NaN     NaN           1             0   \n",
       "1  1.451384     NaN     NaN     NaN           1             0   \n",
       "2  1.735777     NaN     NaN     NaN           1             0   \n",
       "\n",
       "   dataset_activity_id dataset_activity_label  \n",
       "0                    1                  stand  \n",
       "1                    1                  stand  \n",
       "2                    1                  stand  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# STEP 1 — Load & normalize (RWR accel + Loco & ML only)\n",
    "# ============================\n",
    "def _pick_label_ml_then_loco(lo, ml):\n",
    "    \"\"\"\n",
    "    Return (dataset_activity_id:int, dataset_activity_label:str) with ML > Locomotion precedence.\n",
    "    - If ML present and in OPP_ML -> (ml_code, collapsed_label)\n",
    "    - Else if Loco present and in OPP_LOCO (and nonzero) -> (loco_code, simple_label)\n",
    "    - Else -> (UNKNOWN_ID, 'unknown_activity')\n",
    "    \"\"\"\n",
    "    if pd.notna(ml):\n",
    "        m = int(ml)\n",
    "        if m in OPP_ML:\n",
    "            return m, OPP_ML[m]\n",
    "    if pd.notna(lo):\n",
    "        l = int(lo)\n",
    "        if l != 0 and l in OPP_LOCO:\n",
    "            return l, OPP_LOCO[l]\n",
    "    return UNKNOWN_ID, \"unknown_activity\"\n",
    "\n",
    "def load_opportunity_loco_ml(\n",
    "    raw_dir: Path,\n",
    "    drop_all_zero_unknown: bool = True,\n",
    "    drop_allnan_sessions: bool = True,   # NEW: only drop sessions 100% NaN on acc\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load Opportunity++ RWR accelerometer and select labels from (ml_both_arms, locomotion).\n",
    "    Output (RAW STAGE):\n",
    "      subject_id, session_id, timestamp_s,\n",
    "      acc_x/acc_y/acc_z (m/s^2), gyro_* (NaN placeholders),\n",
    "      locomotion (Int64), ml_both_arms (Int64),\n",
    "      dataset_activity_id (Int16, native code), dataset_activity_label (string, collapsed).\n",
    "    \"\"\"\n",
    "    col_names_path = raw_dir / \"column_names.txt\"\n",
    "    if not col_names_path.exists():\n",
    "        print(\"Missing column_names.txt at\", col_names_path)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    names = read_opp_column_names(col_names_path)\n",
    "    files = sorted([p for p in raw_dir.glob(\"*.dat\") if p.is_file()])\n",
    "    if not files:\n",
    "        print(\"No .dat files in\", raw_dir)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    g = 9.80665  # mg → m/s^2\n",
    "\n",
    "    for f in tqdm(files, desc=\"[OPP] files\"):\n",
    "        df = pd.read_csv(f, sep=r\"\\s+\", header=None, names=names, na_values=[\"NaN\"], engine=\"c\")\n",
    "        df = canonicalize_opp_columns(df)\n",
    "        sid, sess = parse_opp_subject_session(f.stem)\n",
    "\n",
    "        # Right-wrist accel — support canonical & legacy names\n",
    "        ax = \"rwr_acc_x\" if \"rwr_acc_x\" in df.columns else (\"rwr_accx\" if \"rwr_accx\" in df.columns else None)\n",
    "        ay = \"rwr_acc_y\" if \"rwr_acc_y\" in df.columns else (\"rwr_accy\" if \"rwr_accy\" in df.columns else None)\n",
    "        az = \"rwr_acc_z\" if \"rwr_acc_z\" in df.columns else (\"rwr_accz\" if \"rwr_accz\" in df.columns else None)\n",
    "\n",
    "        needed = [\"millisec\", \"locomotion\", \"ml_both_arms\"]\n",
    "        missing = [c for c in needed if c not in df.columns]\n",
    "        if ax is None: missing.append(\"rwr_acc_x|rwr_accx\")\n",
    "        if ay is None: missing.append(\"rwr_acc_y|rwr_accy\")\n",
    "        if az is None: missing.append(\"rwr_acc_z|rwr_accz\")\n",
    "        if missing:\n",
    "            raise KeyError(f\"{f.name}: missing required columns: {missing}\")\n",
    "\n",
    "        out = pd.DataFrame({\n",
    "            \"dataset\": \"opportunity++\",\n",
    "            \"subject_id\": sid,\n",
    "            \"session_id\": sess,\n",
    "            \"timestamp_s\": df[\"millisec\"].astype(\"float64\") / 1000.0,\n",
    "\n",
    "            \"acc_x\": df[ax].astype(\"float64\") * (g/1000.0),\n",
    "            \"acc_y\": df[ay].astype(\"float64\") * (g/1000.0),\n",
    "            \"acc_z\": df[az].astype(\"float64\") * (g/1000.0),\n",
    "\n",
    "            # No right-wrist gyro in OPP\n",
    "            \"gyro_x\": np.float32(np.nan),\n",
    "            \"gyro_y\": np.float32(np.nan),\n",
    "            \"gyro_z\": np.float32(np.nan),\n",
    "\n",
    "            # Native tracks we consider\n",
    "            \"locomotion\":   df[\"locomotion\"].astype(\"Int64\"),\n",
    "            \"ml_both_arms\": df[\"ml_both_arms\"].astype(\"Int64\"),\n",
    "        })\n",
    "\n",
    "        # Choose native label (ML > Locomotion), keep native code; collapse only the string label\n",
    "        picked = out[[\"locomotion\",\"ml_both_arms\"]].apply(\n",
    "            lambda r: _pick_label_ml_then_loco(r[\"locomotion\"], r[\"ml_both_arms\"]),\n",
    "            axis=1, result_type=\"expand\"\n",
    "        )\n",
    "        picked.columns = [\"dataset_activity_id\", \"dataset_activity_label\"]\n",
    "        out = pd.concat([out, picked], axis=1)\n",
    "\n",
    "        if drop_all_zero_unknown:\n",
    "            both_zero = (out[\"locomotion\"].fillna(0) == 0) & (out[\"ml_both_arms\"].fillna(0) == 0)\n",
    "            out = out.loc[~(both_zero & (out[\"dataset_activity_label\"] == \"unknown_activity\"))]\n",
    "\n",
    "        rows.append(out)\n",
    "\n",
    "    raw = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "    # === drop only sessions that are COMPLETELY NaN on acc_x/y/z ===\n",
    "    if drop_allnan_sessions and not raw.empty:\n",
    "        sess_all_nan = (\n",
    "            raw.groupby([\"subject_id\",\"session_id\"])[[\"acc_x\",\"acc_y\",\"acc_z\"]]\n",
    "               .apply(lambda g: g.isna().all().all())\n",
    "               .reset_index(name=\"all_acc_nan\")\n",
    "        )\n",
    "        bad = sess_all_nan.loc[sess_all_nan[\"all_acc_nan\"], [\"subject_id\",\"session_id\"]]\n",
    "        if not bad.empty:\n",
    "            bad_idx = pd.MultiIndex.from_frame(bad)\n",
    "            n_before = len(raw)\n",
    "            keep = ~raw.set_index([\"subject_id\",\"session_id\"]).index.isin(bad_idx)\n",
    "            raw = raw.loc[keep].reset_index(drop=True)\n",
    "            print(f\"Dropped accel-empty sessions: {list(bad_idx)} | removed_rows={n_before - len(raw):,}\")\n",
    "        else:\n",
    "            print(\"No accel-empty sessions detected.\")\n",
    "\n",
    "    # NOTE: we intentionally keep partial-NaN rows; STEP 4 will handle interpolation.\n",
    "    any_nan = raw[[\"acc_x\",\"acc_y\",\"acc_z\"]].isna().any().to_dict()\n",
    "    print(f\"Accel NaNs present (kept for later interpolation): {any_nan}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== RAW SUMMARY (Opportunity++ wrist: loco + ML only) ===\")\n",
    "    print(f\"Rows: {len(raw):,}\")\n",
    "    def _est_hz(ts: pd.Series):\n",
    "        arr = ts.to_numpy()\n",
    "        if arr.size < 3: return np.nan\n",
    "        dt = np.diff(arr); dt = dt[(dt > 0) & np.isfinite(dt)]\n",
    "        return float(np.median(1.0/dt)) if dt.size else np.nan\n",
    "    med_hz = raw.groupby([\"subject_id\",\"session_id\"])[\"timestamp_s\"].apply(_est_hz)\n",
    "    print(f\"Median native Hz: {np.nanmedian(med_hz.values):.2f}\")\n",
    "\n",
    "    print(\"\\nTop native labels (collapsed names, native IDs retained):\")\n",
    "    print(raw[\"dataset_activity_label\"].value_counts().head(20))\n",
    "\n",
    "    return raw\n",
    "   \n",
    "# Run STEP 1\n",
    "raw_opp = load_opportunity_loco_ml(RAW_OPP)\n",
    "raw_opp.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc35faee",
   "metadata": {},
   "source": [
    "### Step 2. Map the data and audit the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP 2] Unique raw labels: 15 | Unmapped: 0\n",
      "[STEP 2] coverage=100.00%  (mapped=656,031 / total=656,031)\n",
      "\n",
      "Top mapped globals (up to 10):\n",
      "            mapped_nm  count\n",
      "   posture_stationary 305454\n",
      "    adl_appliance_ops 146825\n",
      "                 walk 144018\n",
      "             adl_food  45901\n",
      "adl_household_general  13833\n",
      "\n",
      "Accel NaNs any={'acc_x': True, 'acc_y': True, 'acc_z': True}  rows_all_acc_nan=15956\n",
      "\n",
      "Dataset-specific (native) labels:\n",
      "['clean_table', 'close_dishwasher', 'close_door', 'close_drawer', 'close_fridge', 'drink_from_cup', 'lie', 'open_dishwasher', 'open_door', 'open_drawer', 'open_fridge', 'sit', 'stand', 'toggle_switch', 'walk']\n",
      "\n",
      "Mapped global labels:\n",
      "['adl_appliance_ops', 'adl_food', 'adl_household_general', 'posture_stationary', 'walk']\n",
      "Raw label unique: 15 | Unmapped: 0\n",
      "Unmapped (top-10):\n",
      "Empty DataFrame\n",
      "Columns: [raw_label, count]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_label</th>\n",
       "      <th>count</th>\n",
       "      <th>mapped_id</th>\n",
       "      <th>mapped_nm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stand</td>\n",
       "      <td>174367</td>\n",
       "      <td>1</td>\n",
       "      <td>posture_stationary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walk</td>\n",
       "      <td>144018</td>\n",
       "      <td>2</td>\n",
       "      <td>walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sit</td>\n",
       "      <td>105692</td>\n",
       "      <td>1</td>\n",
       "      <td>posture_stationary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drink_from_cup</td>\n",
       "      <td>45901</td>\n",
       "      <td>15</td>\n",
       "      <td>adl_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lie</td>\n",
       "      <td>25395</td>\n",
       "      <td>1</td>\n",
       "      <td>posture_stationary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>open_door</td>\n",
       "      <td>25026</td>\n",
       "      <td>20</td>\n",
       "      <td>adl_appliance_ops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>close_door</td>\n",
       "      <td>23065</td>\n",
       "      <td>20</td>\n",
       "      <td>adl_appliance_ops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>open_drawer</td>\n",
       "      <td>20906</td>\n",
       "      <td>20</td>\n",
       "      <td>adl_appliance_ops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>close_drawer</td>\n",
       "      <td>18866</td>\n",
       "      <td>20</td>\n",
       "      <td>adl_appliance_ops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>open_fridge</td>\n",
       "      <td>16288</td>\n",
       "      <td>20</td>\n",
       "      <td>adl_appliance_ops</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_label   count  mapped_id           mapped_nm\n",
       "0           stand  174367          1  posture_stationary\n",
       "1            walk  144018          2                walk\n",
       "2             sit  105692          1  posture_stationary\n",
       "3  drink_from_cup   45901         15            adl_food\n",
       "4             lie   25395          1  posture_stationary\n",
       "5       open_door   25026         20   adl_appliance_ops\n",
       "6      close_door   23065         20   adl_appliance_ops\n",
       "7     open_drawer   20906         20   adl_appliance_ops\n",
       "8    close_drawer   18866         20   adl_appliance_ops\n",
       "9     open_fridge   16288         20   adl_appliance_ops"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# STEP 2 — Mapping audit (raw → global)\n",
    "# ============================\n",
    "# We keep native OPP codes verbatim in dataset_*.\n",
    "# For global_* we map using ACTIVITY_MAP_PATH (RAW2ID via collapsed strings).\n",
    "# Anything unmapped → UNKNOWN_ID / \"other\".\n",
    "\n",
    "# Build (raw_label → mapped global ID/label) summary\n",
    "if raw_opp.empty:\n",
    "    raise SystemExit(\"No Opportunity++ rows after STEP 1. Check RAW_OPP path.\")\n",
    "\n",
    "raw_counts = (\n",
    "    raw_opp[\"dataset_activity_label\"]\n",
    "          .astype(\"string\")\n",
    "          .map(_canon)\n",
    "          .value_counts(dropna=False)\n",
    "          .rename_axis(\"raw_label\")\n",
    "          .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# activity_mapping.json provides RAW2ID keyed by canonical label strings\n",
    "RAW2ID = { _canon(k): int(v) for k, v in ACT_MAP_FULL.get(\"mapping\", {}).items() }\n",
    "ID2NAME = { int(x[\"id\"]): x[\"name\"] for x in ACT_MAP_FULL[\"label_set\"] }\n",
    "\n",
    "raw_counts[\"mapped_id\"] = raw_counts[\"raw_label\"].map(RAW2ID).fillna(UNKNOWN_ID).astype(int)\n",
    "raw_counts[\"mapped_nm\"] = raw_counts[\"mapped_id\"].map(lambda x: ID2NAME.get(int(x), \"other\"))\n",
    "\n",
    "unmapped = raw_counts.loc[raw_counts[\"mapped_id\"] == UNKNOWN_ID]\n",
    "print(f\"[STEP 2] Unique raw labels: {len(raw_counts)} | Unmapped: {len(unmapped)}\")\n",
    "if not unmapped.empty:\n",
    "    print(unmapped.sort_values(\"count\", ascending=False).head(25)[[\"raw_label\",\"count\"]].to_string(index=False))\n",
    "\n",
    "# --- minimal extra checks ---\n",
    "# Coverage\n",
    "total_ct = int(raw_counts[\"count\"].sum())\n",
    "mapped_ct = int(raw_counts.loc[raw_counts[\"mapped_id\"] != UNKNOWN_ID, \"count\"].sum())\n",
    "cov_pct = 100.0 * mapped_ct / max(total_ct, 1)\n",
    "print(f\"[STEP 2] coverage={cov_pct:.2f}%  (mapped={mapped_ct:,} / total={total_ct:,})\")\n",
    "\n",
    "# If there are unmapped labels, show a short list\n",
    "if not unmapped.empty:\n",
    "    print(\"Unmapped (top up to 10):\")\n",
    "    print(unmapped.sort_values(\"count\", ascending=False)\n",
    "                 .head(10)[[\"raw_label\",\"count\"]].to_string(index=False))\n",
    "\n",
    "# Counts per mapped global name (short head)\n",
    "mapped_counts = (\n",
    "    raw_counts.loc[raw_counts[\"mapped_id\"] != UNKNOWN_ID, [\"mapped_nm\",\"count\"]]\n",
    "              .groupby(\"mapped_nm\", as_index=False)[\"count\"].sum()\n",
    "              .sort_values(\"count\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop mapped globals (up to 10):\")\n",
    "print(mapped_counts.head(10).to_string(index=False))\n",
    "\n",
    "# NaN checks on accel\n",
    "acc_nan_any = raw_opp[[\"acc_x\",\"acc_y\",\"acc_z\"]].isna().any().to_dict()\n",
    "all_acc_nan_rows = int(raw_opp[[\"acc_x\",\"acc_y\",\"acc_z\"]].isna().all(axis=1).sum())\n",
    "print(f\"\\nAccel NaNs any={acc_nan_any}  rows_all_acc_nan={all_acc_nan_rows}\")\n",
    "\n",
    "# --- NEW: show dataset-specific vs mapped global label sets ---\n",
    "print(\"\\nDataset-specific (native) labels:\")\n",
    "print(sorted(raw_counts['raw_label'].unique()))\n",
    "\n",
    "print(\"\\nMapped global labels:\")\n",
    "print(sorted(mapped_counts['mapped_nm'].unique()))\n",
    "\n",
    "unmapped = raw_counts.loc[raw_counts[\"mapped_id\"] == UNKNOWN_ID]\n",
    "print(f\"Raw label unique: {len(raw_counts)} | Unmapped: {len(unmapped)}\")\n",
    "print(\"Unmapped (top-10):\")\n",
    "print(unmapped.nlargest(10, \"count\")[[\"raw_label\",\"count\"]].to_string(index=False))\n",
    "raw_counts.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee916e8",
   "metadata": {},
   "source": [
    "### Step 3. Build and clean dataset in stream json fromat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e273e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIFIED (pre-50Hz) rows: 656031\n",
      "         dataset subject_id session_id  timestamp_ns     acc_x     acc_y  \\\n",
      "0  opportunity++         S1       ADL1   98466000000 -4.069760  8.276813   \n",
      "1  opportunity++         S1       ADL1   98499000000 -4.628739  8.963278   \n",
      "2  opportunity++         S1       ADL1   98532000000 -4.148213  8.521978   \n",
      "\n",
      "      acc_z  gyro_x  gyro_y  gyro_z  global_activity_id global_activity_label  \\\n",
      "0  1.470997     NaN     NaN     NaN                   1    posture_stationary   \n",
      "1  1.451384     NaN     NaN     NaN                   1    posture_stationary   \n",
      "2  1.735777     NaN     NaN     NaN                   1    posture_stationary   \n",
      "\n",
      "   dataset_activity_id dataset_activity_label  \n",
      "0                    1                  stand  \n",
      "1                    1                  stand  \n",
      "2                    1                  stand  \n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# STEP 3 — Build schema-ordered continuous_stream (v3)\n",
    "# ==========================================================\n",
    "def to_continuous_stream_opp(\n",
    "    df_raw: pd.DataFrame,\n",
    "    dataset_name: str = \"opportunity++\",\n",
    ") -> pd.DataFrame:\n",
    "    if df_raw.empty:\n",
    "        return pd.DataFrame(columns=[c[\"name\"] for c in SCHEMA[\"columns\"]])\n",
    "\n",
    "    # native → global (map via collapsed dataset_activity_label)\n",
    "    def _canon(s):\n",
    "        if pd.isna(s): return \"\"\n",
    "        s = str(s).strip().lower()\n",
    "        s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "        return s.strip(\"_\")\n",
    "\n",
    "    _RAW2ID = { _canon(k): int(v) for k, v in ACT_MAP_FULL.get(\"mapping\", {}).items() }\n",
    "    _ID2NAME = { int(x[\"id\"]): x[\"name\"] for x in ACT_MAP_FULL[\"label_set\"] }\n",
    "\n",
    "    raw_key = df_raw[\"dataset_activity_label\"].astype(\"string\").map(_canon)\n",
    "    gid     = raw_key.map(_RAW2ID).fillna(UNKNOWN_ID).astype(\"int16\")\n",
    "    glabel  = gid.map(lambda x: _ID2NAME.get(int(x), \"other\")).astype(\"string\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"dataset\":                  dataset_name,\n",
    "        \"subject_id\":               df_raw[\"subject_id\"].astype(\"string\"),\n",
    "        \"session_id\":               df_raw[\"session_id\"].astype(\"string\"),\n",
    "        \"timestamp_ns\":             (df_raw[\"timestamp_s\"].astype(np.float64) * 1e9).round().astype(\"int64\"),\n",
    "\n",
    "        \"acc_x\": df_raw[\"acc_x\"].astype(\"float32\"),\n",
    "        \"acc_y\": df_raw[\"acc_y\"].astype(\"float32\"),\n",
    "        \"acc_z\": df_raw[\"acc_z\"].astype(\"float32\"),\n",
    "        \"gyro_x\": df_raw[\"gyro_x\"].astype(\"float32\"),\n",
    "        \"gyro_y\": df_raw[\"gyro_y\"].astype(\"float32\"),\n",
    "        \"gyro_z\": df_raw[\"gyro_z\"].astype(\"float32\"),\n",
    "\n",
    "        \"global_activity_id\":       gid,\n",
    "        \"global_activity_label\":    glabel,\n",
    "\n",
    "        \"dataset_activity_id\":      df_raw[\"dataset_activity_id\"].astype(\"Int32\"),\n",
    "        \"dataset_activity_label\":   df_raw[\"dataset_activity_label\"].astype(\"string\"),\n",
    "    })\n",
    "\n",
    "    order = [c[\"name\"] for c in SCHEMA[\"columns\"]]\n",
    "    return out[order]\n",
    "\n",
    "opp_df_native = to_continuous_stream_opp(raw_opp)\n",
    "print(\"UNIFIED (pre-50Hz) rows:\", len(opp_df_native))\n",
    "print(opp_df_native.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dd544",
   "metadata": {},
   "source": [
    "### Step 4. Audit check the unified frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12904817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling → 50Hz: 100%|██████████| 23/23 [00:00<00:00, 26.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows @50Hz: 1215455\n",
      "\n",
      "=== QA ===\n",
      "Subjects: 4 | Sessions: 6\n",
      "Monotonic violations (groups): 0\n",
      "Median Hz: 50.00 (target=50)\n",
      "Rows meeting required-not-null: 100.00%\n",
      "Global mapping coverage: 100.0% (unknown=9000)\n",
      "\n",
      "Top-15 canonical labels:\n",
      "global_activity_label\n",
      "posture_stationary       519055\n",
      "walk                     350256\n",
      "adl_appliance_ops        245368\n",
      "adl_food                  77720\n",
      "adl_household_general     23056\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# STEP 4 — Resample → 50 Hz, QA contracts, save\n",
    "# ==================================================\n",
    "def _resample_group_to_50hz(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resample sensors to TARGET_HZ (50).\n",
    "    - Interpolate ONLY acc_x/y/z on a strict 20 ms grid.\n",
    "    - Gyro stays NaN (no RWR gyro in OPP).\n",
    "    - Align labels by nearest-within-half-frame, then ffill/bfill.\n",
    "    - Return schema-ordered frame with required dtypes.\n",
    "    \"\"\"\n",
    "    if g.empty:\n",
    "        return pd.DataFrame(columns=[c[\"name\"] for c in SCHEMA[\"columns\"]])\n",
    "\n",
    "    g = g.sort_values(\"timestamp_ns\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    ACC    = [\"acc_x\",\"acc_y\",\"acc_z\"]\n",
    "    GYRO   = [\"gyro_x\",\"gyro_y\",\"gyro_z\"]\n",
    "    LABELS = [\"global_activity_id\",\"global_activity_label\",\n",
    "              \"dataset_activity_id\",\"dataset_activity_label\"]\n",
    "\n",
    "    # --- 1) Seconds view for acc interpolation ---\n",
    "    tmp = g[[\"timestamp_ns\"] + ACC].copy()\n",
    "    tmp[\"timestamp_s\"] = tmp[\"timestamp_ns\"].to_numpy(dtype=np.float64) / 1e9\n",
    "\n",
    "    up = upsample_df_rate(\n",
    "        df=tmp[[\"timestamp_s\"] + ACC],\n",
    "        tcol=\"timestamp_s\",\n",
    "        num_cols=ACC,\n",
    "        src_hz=30.0,           # helper ignores this if it infers rate; safe placeholder\n",
    "        dst_hz=TARGET_HZ,      # 50\n",
    "    )\n",
    "\n",
    "    # Strict 20 ms grid in ns\n",
    "    up[\"timestamp_ns\"] = (up[\"timestamp_s\"] * 1e9).round().astype(\"int64\")\n",
    "\n",
    "    # --- 2) Label alignment on ns grid ---\n",
    "    HALF_FRAME_NS = int(1e9 / (2 * TARGET_HZ))  # 10 ms\n",
    "    g_lab = g[[\"timestamp_ns\"] + LABELS].copy()\n",
    "    aligned = nearest_label_join_1d(\n",
    "        src_ts_ns=g_lab[\"timestamp_ns\"].to_numpy(),\n",
    "        src_label_df=g_lab[LABELS].copy(),\n",
    "        target_ts_ns=up[\"timestamp_ns\"].to_numpy(),\n",
    "        half_frame_ns=HALF_FRAME_NS,\n",
    "    )\n",
    "\n",
    "    # Guard: fill occasional holes after nearest join\n",
    "    aligned = aligned.ffill().bfill()\n",
    "\n",
    "    # --- 3) Assemble output; add gyro placeholders ---\n",
    "    out = pd.DataFrame({\n",
    "        \"dataset\":    g[\"dataset\"].iloc[0],\n",
    "        \"subject_id\": g[\"subject_id\"].iloc[0],\n",
    "        \"session_id\": g[\"session_id\"].iloc[0],\n",
    "    }, index=up.index)\n",
    "\n",
    "    out[\"timestamp_ns\"] = up[\"timestamp_ns\"].astype(\"int64\")\n",
    "    out[ACC] = up[ACC].astype(\"float32\")\n",
    "\n",
    "    for c in GYRO:\n",
    "        out[c] = np.nan\n",
    "    out[GYRO] = out[GYRO].astype(\"float32\")\n",
    "\n",
    "    out[LABELS] = aligned.astype({\n",
    "        \"global_activity_id\":    \"int16\",\n",
    "        \"global_activity_label\": \"string\",\n",
    "        \"dataset_activity_id\":   \"Int32\",\n",
    "        \"dataset_activity_label\":\"string\",\n",
    "    })\n",
    "\n",
    "    # --- 4) Schema order + required-not-null hardening ---\n",
    "    order = [c[\"name\"] for c in SCHEMA[\"columns\"]]\n",
    "    out = out[order]\n",
    "    out[\"dataset\"]    = out[\"dataset\"].astype(\"string\").fillna(\"opportunity++\")\n",
    "    out[\"subject_id\"] = out[\"subject_id\"].astype(\"string\").fillna(\"S?\")\n",
    "    out[\"session_id\"] = out[\"session_id\"].astype(\"string\").fillna(\"UNK\")\n",
    "    return out\n",
    "\n",
    "# Run resampling per (subject, session)\n",
    "groups = [\"subject_id\",\"session_id\"]\n",
    "out_chunks = []\n",
    "for (_sid, _sess), g in tqdm(opp_df_native.groupby(groups, sort=False), desc=\"Resampling → 50Hz\"):\n",
    "    out_chunks.append(_resample_group_to_50hz(g))\n",
    "\n",
    "opp_50 = pd.concat(out_chunks, ignore_index=True)\n",
    "print(\"Rows @50Hz:\", len(opp_50))\n",
    "\n",
    "# ---- QA contracts ----\n",
    "print(\"\\n=== QA ===\")\n",
    "print(\"Subjects:\", opp_50[\"subject_id\"].nunique(), \"| Sessions:\", opp_50[\"session_id\"].nunique())\n",
    "\n",
    "# Monotonic per group\n",
    "viol = 0\n",
    "for (_sid, _sess), g in opp_50.groupby(groups, sort=False):\n",
    "    ts = g[\"timestamp_ns\"].to_numpy()\n",
    "    if ts.size and not np.all(np.diff(ts) >= 0):\n",
    "        viol += 1\n",
    "print(\"Monotonic violations (groups):\", viol)\n",
    "\n",
    "def est_hz_ns(ts_ns: pd.Series):\n",
    "    arr = ts_ns.to_numpy()\n",
    "    if arr.size < 3: return np.nan\n",
    "    dt = np.diff(arr) / 1e9\n",
    "    dt = dt[(dt > 0) & np.isfinite(dt)]\n",
    "    return float(np.median(1.0/dt)) if dt.size else np.nan\n",
    "\n",
    "hz = opp_50.groupby(groups)[\"timestamp_ns\"].apply(est_hz_ns)\n",
    "print(f\"Median Hz: {np.nanmedian(hz.values):.2f} (target={SCHEMA['rate_hz']})\")\n",
    "\n",
    "req = SCHEMA[\"expectations\"][\"required_not_null\"]\n",
    "pct = opp_50[req].notnull().all(axis=1).mean() * 100\n",
    "print(f\"Rows meeting required-not-null: {pct:.2f}%\")\n",
    "\n",
    "cov = (opp_50[\"global_activity_id\"] != UNKNOWN_ID).mean() * 100\n",
    "print(f\"Global mapping coverage: {cov:.1f}% (unknown={UNKNOWN_ID})\")\n",
    "\n",
    "print(\"\\nTop-15 canonical labels:\")\n",
    "print(opp_50[\"global_activity_label\"].value_counts().head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597d729",
   "metadata": {},
   "source": [
    "### Step 5. Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9efe1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → /home/aidan/IMU_LM_Data/data/cleaned_premerge/opportunity_clean_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Optional save\n",
    "out_path = CLEANED / \"opportunity_clean_data.parquet\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "opp_50.to_parquet(out_path, index=False)\n",
    "print(\"Saved →\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".IMU_Data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
